import pprint
import os
import yaml
import sys
from directories import *
from Bio import SeqIO
from snakemake.utils import Paramspace, min_version
import pandas as pd


min_version("6.0")

yaml.warnings({'YAMLLoadWarning': False})
shell.executable("/bin/bash")

SAMPLES = {}
with open(config["sample_sheet"]) as sample_sheet_file:
    SAMPLES = yaml.safe_load(sample_sheet_file)

mincov = 30

def Get_Ref_header(reffile):
    return [record.id for record in SeqIO.parse(reffile, "fasta")]

def construct_paramspace(sampleinfo):
    space = []
    for key, val in sampleinfo.items():
    
        if val["MATCH-REF"] is False:
            for id in Get_Ref_header(val["REFERENCE"]):
                space.append({"Virus": val["VIRUS"], "RefID": id, "Sample": key})
    return Paramspace(pd.DataFrame.from_dict(space))

paramspace = construct_paramspace(SAMPLES)

def construct_all_rule(sampleinfo):
    files = set()
    files.add(f"{res}multiqc.html")

    for key, val in sampleinfo.items():
        if val["MATCH-REF"] is False:
            for id in Get_Ref_header(val["REFERENCE"]):
                files.add(f"{res}{val['VIRUS']}/{id}/consensus.fasta")
                files.add(f"{res}{val['VIRUS']}/{id}/mutations.tsv")
                files.add(f"{res}{val['VIRUS']}/{id}/Width_of_coverage.tsv")
                if val["PRIMERS"] is not None:
                    files.add(f"{res}{val['VIRUS']}/{id}/Amplicon_coverage.tsv")

    return list(files)

def low_memory_job(wildcards, threads, attempt):
    if config['computing_execution'] == 'local':
        return min(attempt * threads * 1 * 1000, config['max_local_mem'])
    return attempt * threads * 1 * 1000

def medium_memory_job(wildcards, threads, attempt):
    if config['computing_execution'] == 'local':
        return min(attempt * threads * 2 * 1000, config['max_local_mem'])
    return attempt * threads * 2 * 1000

def high_memory_job(wildcards, threads, attempt):
    if config['computing_execution'] == 'local':
        return min(attempt * threads * 4 * 1000, config['max_local_mem'])
    return attempt * threads * 4 * 1000

def construct_MultiQC_input(_wildcards):
    if config['platform'] == "nanopore" or config['platform'] == "iontorrent":
        pre = expand(
            f"{datadir}{qc_pre}{{param_struct}}_fastqc.zip", param_struct=paramspace.instance_patterns)
    elif config['platform'] == "illumina":
        pre = expand(
            f"{datadir}{qc_pre}{{param_struct}}_{read}_fastqc.zip",
            param_struct=paramspace.instance_patterns,
            read = "R1 R2".split()
            )
    else:
        raise ValueError(f"Platform {config['platform']} not recognised. Choose one of [illumina, nanopore, iontorrent].")
    post = expand(f"{datadir}{qc_post}{{param_struct}}_fastqc.zip", param_struct=paramspace.instance_patterns)

    #print(list(set(pre)))
    return pre + post

localrules:
    all,
    prepare_refs,
    copy_qc

rule all:
    input: #construct_all_rule(SAMPLES)
        f"{res}multiqc.html"


rule prepare_refs:
    input: lambda wildcards: SAMPLES[str(wildcards.sample).split('~')[1]]["REFERENCE"]
    output: f"{datadir}{refdir}{{target}}/{{refID}}/{{sample}}_reference.fasta"
    params:
        script=srcdir("scripts/extract_ref.py")
    shell:
        "python {params.script} {input} {output} {wildcards.refID}"

if config["platform"] in ["nanopore", "iontorrent"]:
    p1_mapping_settings = "-ax sr" if config['platform'] == "iontorrent" else '-ax map-ont'

    rule qc_raw:
        input: lambda wildcards: SAMPLES[str(wildcards.sample).split('~')[1]]["INPUTFILE"]
        output:
            html = temp(f"{datadir}{qc_pre}" + "{target}--{refID}--{sample}_fastqc.html"),
            zip  = temp(f"{datadir}{qc_pre}" + "{target}--{refID}--{sample}_fastqc.zip"),
        conda: f"{conda_envs}Clean.yaml"
        log: f"{logdir}QC_raw_data_" + "{target}.{refID}.{sample}.log"
        benchmark: f"{logdir}{bench}QC_raw_data_" + "{target}.{refID}.{sample}.txt"
        threads: config['threads']['QC']
        resources: mem_mb = low_memory_job
        params:
            output_dir = f"{datadir}{qc_pre}",
            script = srcdir("scripts/fastqc_wrapper.sh")
        shell:
            """
            bash {params.script} {input} {params.output_dir} {output.html} {output.zip} {log}
            """

    rule remove_adapters_p1:
        input:
            ref = f"{datadir}{refdir}{{target}}/{{refID}}/{{sample}}_reference.fasta",
            fq = lambda wildcards: SAMPLES[str(wildcards.sample).split('~')[1]]["INPUTFILE"]
        output:
            bam = f"{datadir}{{target}}/{{refID}}/{{sample}}/{cln}{raln}raw_aln.bam",
            index = f"{datadir}{{target}}/{{refID}}/{{sample}}/{cln}{raln}raw_aln.bam.bai",
        conda: f"{conda_envs}Alignment.yaml"
        log: f"{logdir}RemoveAdapters_p1_" + "{target}.{refID}.{sample}.log"
        benchmark: f"{logdir}{bench}RemoveAdapters_p1_" + "{target}.{refID}.{sample}.txt"
        threads: config['threads']['Alignments']
        resources: mem_mb = medium_memory_job
        params:
            mapthreads = config['threads']['Alignments'] -1,
            filters = config['runparams']['alignmentfilters'],
            mapping_settings = p1_mapping_settings,
        shell:
            """
            minimap2 {params.mapping_settings} -t {params.mapthreads} {input.ref} {input.fq} 2>> {log} |\
            samtools view -@ {threads} {params.filters} -uS 2>> {log} |\
            samtools sort -o {output.bam} >> {log} 2>&1
            samtools index {output.bam} >> {log} 2>&1
            """

rule remove_adapters_p2:
    input: rules.remove_adapters_p1.output.bam
    output: f"{datadir}{{target}}/{{refID}}/{cln}{noad}{{sample}}.fastq"
    conda: f"{conda_envs}Clean.yaml"
    threads: config['threads']['AdapterRemoval']
    resources: mem_mb = low_memory_job
    params: script = srcdir('scripts/clipper.py')
    shell:
        """
        python {params.script} --input {input} --output {output} --threads {threads}
        """

rule qc_filter:
    input: rules.remove_adapters_p2.output
    output:
        fq = f"{datadir}{{target}}/{{refID}}/{cln}{qcfilt}{{sample}}.fastq",
        html= f"{datadir}{{target}}/{{refID}}/{cln}{qcfilt}{html}{{sample}}_fastqc.html",
        json= f"{datadir}{{target}}/{{refID}}/{cln}{qcfilt}{json}{{sample}}_fastqc.json",
    conda: f"{conda_envs}Clean.yaml"
    log: f"{logdir}QC_filter_" + "{target}.{refID}.{sample}.log"
    benchmark: f"{logdir}{bench}QC_filter_" + "{target}.{refID}.{sample}.txt"
    threads: config['threads']['QC']
    resources: mem_mb = low_memory_job
    params:
        score = config['runparams'][f"qc_filter_{config['platform']}"],
        size = config['runparams'][f"qc_window_{config['platform']}"],
        length = config['runparams']['qc_min_readlength']
    shell:
        """
        fastp --thread {threads} -i {input} \
            -A -Q --cut_right \
            --cut_right_mean_quality {params.score} \
            --cut_right_window_size {params.size} \
            -l {params.length} -o {output.fq} \
            -h {output.html} -j {output.json} > {log} 2>&1
        """

rule qc_clean:
    input: rules.qc_filter.output.fq
    output:
        html=temp(f"{datadir}{qc_post}" + "{target}--{refID}--{sample}_fastqc.html"),
        zip=temp(f"{datadir}{qc_post}" + "{target}--{refID}--{sample}_fastqc.zip"),
    conda: f"{conda_envs}Clean.yaml"
    log: f"{logdir}QC_clean_" + "{target}.{refID}.{sample}.log"
    benchmark: f"{logdir}{bench}QC_clean_" + "{target}.{refID}.{sample}.txt"
    threads: config['threads']['QC']
    resources: mem_mb = low_memory_job
    params:
        outdir = f"{datadir}{qc_post}",
        script = srcdir("scripts/fastqc_wrapper.sh")
    shell:
        """
        bash {params.script} {input} {params.outdir} {output.html} {output.zip} {log}
        """

rule copy_qc:
    input:
        raw=rules.qc_raw.output.zip,
        clean=rules.qc_clean.output.zip
    output:
        raw=f"{datadir}{qc_pre}{{target}}/{{refID}}/{{sample}}_fastqc.zip",
        clean=f"{datadir}{qc_post}{{target}}/{{refID}}/{{sample}}_fastqc.zip"
    shell:
        """
        cp {input.raw} {output.raw}
        cp {input.clean} {output.clean}
        """

rule multiqc_report:
    input: construct_MultiQC_input
    output:
        f"{res}multiqc.html",
        expand(f"{res}{mqc_data}multiqc_{{program}}.txt", program = "fastqc")
    conda: f"{conda_envs}Clean.yaml"
    log: f"{logdir}MultiQC_report.log"
    benchmark: f"{logdir}{bench}MultiQC_report.txt"
    threads: 1
    resources: mem_mb = medium_memory_job
    params:
        conffile = srcdir('files/multiqc_config.yaml'),
        outdir = res
    shell:
        """
        multiqc -d --force --config {params.conffile} -o {params.outdir} -n multiqc.html {input} > {log} 2>&1
        """